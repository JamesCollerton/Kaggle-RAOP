dataframe_list[[which(dataframe_list$component == "A")]]
[which(dataframe_list$component == "A")
which(dataframe_list$component == "A")
split(test, component)
dataframe_list <- split(test,component)
dataframe_list$A
library(seqinr)
x = read.alignment("proteins.fasta",format="fasta",forceToLower=FALSE)
mylist = x$seq
list.2 = strsplit(mylist, split = NULL)
lapply(list.2, "[", 10) == "L"
install.packages("seqinr")
library(seqinr)
x = read.alignment("proteins.fasta",format="fasta",forceToLower=FALSE)
mylist = x$seq
list.2 = strsplit(mylist, split = NULL)
lapply(list.2, "[", 10) == "L"
proteins.fasta
proteins
data(proteins)
data(proteins.fasta)
el_1 <- c(0, 1, 2, 3)
el_2 <- el_1
el_3 <- el_1
seq <- c("aaaaaaaaaa", "bbbbbbbbbbb", "cccccccccc", "dddddddddd")
x <- data.frame(el_1, el_2, el_3, seq)
x
mylist = x$seq
mylist
mylist[1]
list.2 = strsplit(mylist, split = NULL)
mylist[[1]]
list.2 = strsplit(seq, split = NULL)
list.2
x <- data.frame(c(el_1, el_2, el_3, seq))
x
x <- data.frame(el_1, el_2, el_3, seq)
x
x$seq
x <- data.frame(el_1, el_2, el_3, seq, stringsAsFactors = FALSE)
x$seq
x <- data.frame(el_1, el_2, el_3, seq, stringsAsFactors = FALSE)
mylist = x$seq
list.2 = strsplit(mylist, split = NULL)
list.2
lapply(list.2, "[", 10) == "L"
lapply(list.2, "[", 10) == "b"
a1 <- c(0, 1, 2)
a2 <- c(3, 4, 5)
a3 <- c(6, 7, 8)
csv <- data.frame(a1, a2, a3)
csv
csv[,c("a1", "a3")]
fibonnaci(num_initial_steps){
if(num_initial_steps == 0){ return(0) }
else if(num_initial_steps == 1){ return(1) }
else{ return(fibonacci(num_initial_steps - 1) + fibonacci(num_initial_steps - 2)) }
}
fibonnaci_reg <- function(num_initial_steps){
if(num_initial_steps == 0){ return(0) }
else if(num_initial_steps == 1){ return(1) }
else{ return(fibonacci(num_initial_steps - 1) + fibonacci(num_initial_steps - 2)) }
}
fibonnaci_reg(7)
fibonnaci_reg <- function(num_initial_steps){
if(num_initial_steps == 0){ return(0) }
else if(num_initial_steps == 1){ return(1) }
else{ return(fibonacci_reg(num_initial_steps - 1) + fibonacci_reg(num_initial_steps - 2)) }
}
fibonnaci_reg(7)
fibonacci_reg <- function(num_initial_steps){
if(num_initial_steps == 0){ return(0) }
else if(num_initial_steps == 1){ return(1) }
else{ return(fibonacci_reg(num_initial_steps - 1) + fibonacci_reg(num_initial_steps - 2)) }
}
fibonacci_reg(7)
fibonacci_reg <- function(fib_vec, num_initial_steps){
if(num_initial_steps == 0){
fib_vec[length(fib_vec) + 1] <- 0
return(0)
}
else if(num_initial_steps == 1){
fib_vec[length(fib_vec) + 1] <- 1
return(1)
}
else{
fib_vec[length(fib_vec) + 1] <- num_initial_steps
return(fibonacci_reg(num_initial_steps - 1) + fibonacci_reg(num_initial_steps - 2))
}
}
fibonacci_reg <- function(fib_vec, num_initial_steps){
if(num_initial_steps == 0){
fib_vec[length(fib_vec) + 1] <- 0
return(0)
}
else if(num_initial_steps == 1){
print(fib_vec)
fib_vec[length(fib_vec) + 1] <- 1
return(1)
}
else{
fib_vec[length(fib_vec) + 1] <- num_initial_steps
return(fibonacci_reg(num_initial_steps - 1) + fibonacci_reg(num_initial_steps - 2))
}
}
fibonacci_reg(c(), 7)
empty_vec <- c()
fibonacci_reg(empty_vec, 7)
fibonacci_reg <- function(fib_vec, num_initial_steps){
if(num_initial_steps == 0){
fib_vec[length(fib_vec) + 1] <- 0
return(0)
}
else if(num_initial_steps == 1){
print(fib_vec)
fib_vec[length(fib_vec) + 1] <- 1
return(1)
}
else{
fib_vec[length(fib_vec) + 1] <- num_initial_steps
return(fibonacci_reg(fib_vec, num_initial_steps - 1) + fibonacci_reg(fib_vec, num_initial_steps - 2))
}
}
fibonacci_reg(empty_vec, 7)
difftime(as.Date("2009-10-01 10:00"), as.Date("2009-10-01 9:00"), "hours")
difftime(as.Date("2009-10-01 10:00"), as.Date("2009-10-01 9:00"), units = "hours")
as.Date("2009-10-01 10:00"
as.Date("2009-10-01 10:00")
as.Date("2009-10-01 10:00")
as.POSIXct("2009-10-01 10:00", format="%m/%d/%y %H:%M")
as.POSIXct("2009-10-01 10:00", format="%y/%n/%d %H:%M")
as.POSIXct("2009-10-01 10:00", format="%y-%n-%d %H:%M")
as.POSIXt("2009-10-01 10:00", format="%y-%n-%d %H:%M")
as.POSIXct("2009-10-01 10:00", format="%y-%n-%d %H:%M")
as.POSIXct("2009-10-01 10:00", format="%yyyy-%mm-%dd %HH:%MM")
as.POSIXct(strptime("2011-03-27 01:30:00", "%Y-%m-%d %H:%M:%S"))
as.POSIXct(strptime("2009-10-01 01:30:00", "%Y-%m-%d %H:%M:%S"))
as.POSIXct(strptime("2009-10-01 01:30:00", "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime("2009-09-01 01:30:00", "%Y-%m-%d %H:%M:%S"))
as.POSIXct(strptime("2009-10-01 01:30:00", "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime("2009-09-01 01:29:00", "%Y-%m-%d %H:%M:%S"))
as.POSIXct(strptime("2009-10-01 01:30:00", "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime("2009-09-01 01:28:00", "%Y-%m-%d %H:%M:%S"))
as.POSIXct(strptime("2009-10-01 01:30:00", "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime("2009-09-01 01:28:00", "%Y-%m-%d %H:%M:%S"))
as.POSIXct(strptime("2009-10-01 10:00:00", "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime("2009-09-01 09:00:00", "%Y-%m-%d %H:%M:%S"))
as.POSIXct(strptime("2009-10-01 10:00:00", "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime("2009-10-01 09:00:00", "%Y-%m-%d %H:%M:%S"))
as.POSIXct(strptime("29/12/2014 10:00:00", "%d/%m/%Y %H:%M:%S")) - as.POSIXct(strptime("29/12/2014 08:00:00", "%d/%m/%Y %H:%M:%S"))
as.POSIXct("2009-10-01 10:00:00", "%Y-%m-%d %H:%M:%S") -
as.POSIXct("2009-10-01 09:00:00", "%Y-%m-%d %H:%M:%S")
as.POSIXct("2009-10-01 10:00:00", "%Y-%m-%d %H:%M:%S") -
as.POSIXct("2009-10-01 09:00:00", "%Y-%m-%d %H:%M:%S")
Date <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value <- c(0, 1, 2)
dateDataframe <- as.dataframe(Date, Value)
dateDataframe <- as.data.frame(Date, Value)
dateDataframe <- as.data.frame(c(Date, Value))
Date <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value <- c(0, 1, 2)
dateDataframe <- as.data.frame(c(Date, Value))
dateDataframe
dateDataframe <- as.data.frame(Date, Value)
Date_Col <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value_Col <- c(0, 1, 2)
dateDataframe <- as.data.frame(Date_Col, Value_Col)
Date_Col <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value_Col <- c(0, 1, 2)
dateDataframe <- as.data.frame(Date_Col, Value_Col, row.names = NULL)
dateDataframe <- data.frame(Date_Col, Value_Col)
Date <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value <- c(0, 1, 2)
dateDataframe <- data.frame(Date, Value)
dateDataframe
dateDataframe%dateDifferences <- diff(dateDataframe$Date)
diff(dateDataframe$Date)
dateDifferences <- as.POSIXct(strptime(dateDataframe$Date, "%Y-%m-%d %H:%M:%S")) - as.POSIXct(strptime(dateDataframe$Date, "%Y-%m-%d %H:%M:%S"))
dateDifferences
dateDifferences <- as.POSIXct(strptime(dateDataframe$Date[2:nrow(dateDataframe)], "%Y-%m-%d %H:%M:%S")) - as.POSIXct(strptime(dateDataframe$Date[1:nrow(dateDataframe) - 1], "%Y-%m-%d %H:%M:%S"))
dateDifferences
Date <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value <- c(0, 1, 2)
dateDataframe <- data.frame(Date, Value)
dateDifferences <- c(0,
as.POSIXct(strptime(dateDataframe$Date[1:nrow(dateDataframe) - 1], "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime(dateDataframe$Date[2:nrow(dateDataframe)], "%Y-%m-%d %H:%M:%S")) )
Date <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value <- c(0, 1, 2)
dateDataframe <- data.frame(Date, Value)
dateDataframe$dateDifferences <- c(0,
as.POSIXct(strptime(dateDataframe$Date[1:nrow(dateDataframe) - 1], "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime(dateDataframe$Date[2:nrow(dateDataframe)], "%Y-%m-%d %H:%M:%S")) )
dateDataframe
Date <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value <- c(0, 1, 2)
dateDataframe <- data.frame(Date, Value)
dateDataframe$dateDifferences <- c(0,
as.string(
as.POSIXct(strptime(dateDataframe$Date[1:nrow(dateDataframe) - 1], "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime(dateDataframe$Date[2:nrow(dateDataframe)], "%Y-%m-%d %H:%M:%S")) ) )
Date <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value <- c(0, 1, 2)
dateDataframe <- data.frame(Date, Value)
dateDataframe$dateDifferences <- c(0,
toString(
as.POSIXct(strptime(dateDataframe$Date[1:nrow(dateDataframe) - 1], "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime(dateDataframe$Date[2:nrow(dateDataframe)], "%Y-%m-%d %H:%M:%S")) ) )
Date <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value <- c(0, 1, 2)
dateDataframe <- data.frame(Date, Value)
dateDataframe$dateDifferences <- c(0,
(
as.POSIXct(strptime(dateDataframe$Date[1:nrow(dateDataframe) - 1], "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime(dateDataframe$Date[2:nrow(dateDataframe)], "%Y-%m-%d %H:%M:%S")) ) )
Date <- c("2009-10-01 10:00:00", "2009-10-01 09:00:00", "2009-10-01 08:00:00")
Value <- c(0, 1, 2)
dateDataframe <- data.frame(Date, Value)
dateDataframe$dateDifferences <- c(0,
as.POSIXct(strptime(dateDataframe$Date[1:nrow(dateDataframe) - 1], "%Y-%m-%d %H:%M:%S")) -
as.POSIXct(strptime(dateDataframe$Date[2:nrow(dateDataframe)], "%Y-%m-%d %H:%M:%S")) )
dateDataframe$dateDifferences[1]
dateDataframe$dateDifferences[2]
dateDataframe$dateDifferences[3]
source('~/.active-rstudio-document', echo=TRUE)
plot(BabyECG, col = "cadetblue", type = "l", main = "Baby ECG")
BabyECGwd <- wd(BabyECG, filter.number=10, family="DaubLeAsymm", type="wavelet",
bc="periodic", verbose=FALSE)
#Thresholded BabyECG
BabyECGthresh <- threshold.wd(BabyECGwd, levels = c(10, 9, 8, 7, 6, 5),
type = "hard", policy = "universal", by.level = "FALSE")
#Inverse DWT of BabyECG
fhat <- wr.wd(BabyECGthresh, start.level = 0)
plot(fhat, col = "cadetblue", type = "l", main = "Baby ECG", lwd = 1.5,
ylab = " ")
#Sets working directory to the relevant space and brings up the appropriate library.
# required_packages <- c("tm", "rjson", "KernSmooth", "SnowballC", "wordcloud", "httr", "XML", "RCurl")
# install.packages(required_packages)
install.packages("RCurl")
library('tm')
library("rjson")
library("KernSmooth")
library('SnowballC')
library('wordcloud')
library('httr')
library('XML')
library('RCurl')
setwd("~/Documents/Programming/Git_Folders/Kaggle-RAOP/Updated_Attempt/Data")
#Reads in the relevant .json file
meta_data <- fromJSON(file = "train.json")
#This part just makes a nice dataframe and gives nice titles.
meta_dataframe <- data.frame("giver_username_if_known" = unlist(lapply(meta_data, function(l) l[[1]])),
"number_of_downvotes_of_request_at_retrieval" = unlist(lapply(meta_data, function(l) l[[2]])),
"number_of_upvotes_of_request_at_retrieval" = unlist(lapply(meta_data, function(l) l[[3]])),
"post_was_edited" = unlist(lapply(meta_data, function(l) l[[4]])),
"request_id" = unlist(lapply(meta_data, function(l) l[[5]])),
"request_number_of_comments_at_retrieval" = unlist(lapply(meta_data, function(l) l[[6]])),
"request_text" = unlist(lapply(meta_data, function(l) l[[7]])),
"request_text_edit_aware" = unlist(lapply(meta_data, function(l) l[[8]])),
"request_title" = unlist(lapply(meta_data, function(l) l[[9]])),
"requester_account_age_in_days_at_request" = unlist(lapply(meta_data, function(l) l[[10]])),
"requester_account_age_in_days_at_retrieval" = unlist(lapply(meta_data, function(l) l[[11]])),
"requester_days_since_first_post_on_raop_at_request" = unlist(lapply(meta_data, function(l) l[[12]])),
"requester_days_since_first_post_on_raop_at_retrieval" = unlist(lapply(meta_data, function(l) l[[13]])),
"requester_number_of_comments_at_request" = unlist(lapply(meta_data, function(l) l[[14]])),
"requester_number_of_comments_at_retrieval" = unlist(lapply(meta_data, function(l) l[[15]])),
"requester_number_of_comments_in_raop_at_request" = unlist(lapply(meta_data, function(l) l[[16]])),
"requester_number_of_comments_in_raop_at_retrieval" = unlist(lapply(meta_data, function(l) l[[17]])),
"requester_number_of_posts_at_request" = unlist(lapply(meta_data, function(l) l[[18]])),
"requester_number_of_posts_at_retrieval" = unlist(lapply(meta_data, function(l) l[[19]])),
"requester_number_of_posts_on_raop_at_request" = unlist(lapply(meta_data, function(l) l[[20]])),
"requester_number_of_posts_on_raop_at_retrieval" = unlist(lapply(meta_data, function(l) l[[21]])),
"requester_number_of_subreddits_at_request" = unlist(lapply(meta_data, function(l) l[[22]])),
"requester_received_pizza" = unlist(lapply(meta_data, function(l) l[[23]])),
"requester_upvotes_minus_downvotes_at_request" = unlist(lapply(meta_data, function(l) l[[25]])),
"requester_upvotes_minus_downvotes_at_retrieval" = unlist(lapply(meta_data, function(l) l[[26]])),
"requester_upvotes_plus_downvotes_at_request" = unlist(lapply(meta_data, function(l) l[[27]])),
"requester_upvotes_plus_downvotes_at_retrieval" = unlist(lapply(meta_data, function(l) l[[28]])),
"requester_username" = unlist(lapply(meta_data, function(l) l[[30]])),
"unix_timestamp_of_request" = unlist(lapply(meta_data, function(l) l[[31]])),
"unix_timestamp_of_request_utc" = unlist(lapply(meta_data, function(l) l[[32]])))
###################################################################
#Exploratory analysis##############################################
###################################################################
###################################################################
#The first trend to be explored is if having a lot of downvotes would lead to a dimished
#probability of recieving pizza.
###################################################################
check_voting_impact <- function(vote_type, sufficient_data_threshold)
{
votes <- data.frame(meta_dataframe$requester_received_pizza,
vote_type)
vote_table <- table(votes)
false_sufficient_data <- which(vote_table["FALSE",] > sufficient_data_threshold)
true_sufficient_data <- which(vote_table["TRUE",] > sufficient_data_threshold)
sufficient_data <- intersect(false_sufficient_data, true_sufficient_data)
vote_table <- vote_table[,sufficient_data]
xvals <- unique(as.numeric(colnames(vote_table)))
yvals <- vote_table["TRUE",]/(vote_table["TRUE",] + vote_table["FALSE",])
plot(xvals, yvals, col = "dodgerblue", xlab = "Number of Downvotes",
ylab = "Proportion of Successful Pizza Requests", main = "Corellation between Number of Votes
and Proportion of Successful Pizza Requests", pch = 16)
#Fits a linear model to the data whilst ignoring all values that jet off to infinity.
lm_fit_df <- data.frame(xvals, yvals)
lm_fit_df <- lm_fit_df[which(lm_fit_df$yvals != Inf),]
fit <- lm(lm_fit_df$yvals ~ lm_fit_df$xvals)
abline(data.frame(summary(fit)$coefficients)$Estimate, col = "firebrick1", lwd = 1.5)
# Finally look at Pearson's.
cor(yvals, xvals, method = "pearson")
}
check_voting_impact(meta_dataframe$number_of_downvotes_of_request_at_retrieval, 7)
check_voting_impact(meta_dataframe$number_of_upvotes_of_request_at_retrieval, 8)
# So now we have shown that, when you ignore posts with sparse information, there is
# a strong correlation between upvotes and pizza, and a weak one between downvotes.
# ------------------------------------------------------------------------------------------------
create_word_cloud <- function(requests){
#   titles <- requests$request_title
new_titles <- list()
for (i in 1:length(titles)){
new_titles[[i]] <- unlist(strsplit(as.vector(titles[i]), " "))
}
all_title_words = unlist(new_titles)
all_title_words_table_df <- data.frame(table(all_title_words))
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
words_page <- "https://en.wikipedia.org/wiki/Most_common_words_in_English"
tables <- readHTMLTable(words_page)
useful_words <- tables$'NULL'$V2
useful_words = c(useful_words, "is", "Pizza", "pizza", "[Request]", "[REQUEST]", "[request]", "-")
all_title_words_table_df <- all_title_words_table_df[-which(all_title_words_table_df$all_title_words %in% useful_words),]
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
#This gives us something to look at and play with. Now we try using the text mining package to
#create a word cloud.
for_word_cloud_corpus <- Corpus(VectorSource(all_title_words_table_df$all_title_words))
#cleaning
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stripWhitespace)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, tolower)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, PlainTextDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, stopwords("english"))
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stemDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, c("[request]", "request"))
wordcloud(for_word_cloud_corpus, scale = c(4, 1), max.words = 100, colors = brewer.pal(8, "Dark2"))
}
successful_requests <- meta_dataframe[which(meta_dataframe$requester_received_pizza == TRUE),]$titles
failed_requests <- meta_dataframe[which(meta_dataframe$requester_received_pizza == FALSE),]$titles
create_word_cloud(successful_requests)
create_word_cloud(failed_requests)
create_word_cloud <- function(titles){
#   titles <- requests$request_title
new_titles <- list()
for (i in 1:length(titles)){
new_titles[[i]] <- unlist(strsplit(as.vector(titles[i]), " "))
}
all_title_words = unlist(new_titles)
all_title_words_table_df <- data.frame(table(all_title_words))
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
words_page <- "https://en.wikipedia.org/wiki/Most_common_words_in_English"
tables <- readHTMLTable(words_page)
useful_words <- tables$'NULL'$V2
useful_words = c(useful_words, "is", "Pizza", "pizza", "[Request]", "[REQUEST]", "[request]", "-")
all_title_words_table_df <- all_title_words_table_df[-which(all_title_words_table_df$all_title_words %in% useful_words),]
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
#This gives us something to look at and play with. Now we try using the text mining package to
#create a word cloud.
for_word_cloud_corpus <- Corpus(VectorSource(all_title_words_table_df$all_title_words))
#cleaning
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stripWhitespace)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, tolower)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, PlainTextDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, stopwords("english"))
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stemDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, c("[request]", "request"))
wordcloud(for_word_cloud_corpus, scale = c(4, 1), max.words = 100, colors = brewer.pal(8, "Dark2"))
}
successful_requests <- meta_dataframe[which(meta_dataframe$requester_received_pizza == TRUE),]$titles
failed_requests <- meta_dataframe[which(meta_dataframe$requester_received_pizza == FALSE),]$titles
create_word_cloud(successful_requests)
create_word_cloud(failed_requests)
successful_requests
successful_requests <- meta_dataframe[which(meta_dataframe$requester_received_pizza == TRUE),"titles"]
successful_requests
successful_requests <- meta_dataframe[which(meta_dataframe$requester_received_pizza == TRUE),]
successful_requests
successful_requests$title
successful_requests$titles
successful_requests <- meta_dataframe[which(meta_dataframe$requester_received_pizza == TRUE),]$request_title
failed_requests <- meta_dataframe[which(meta_dataframe$requester_received_pizza == FALSE),]$request_title
create_word_cloud(successful_requests)
create_word_cloud(failed_requests)
create_word_cloud(successful_requests_titles)
successful_requests_titles <- meta_dataframe[which(meta_dataframe$requester_received_pizza == TRUE),]$request_title
failed_requests_titles <- meta_dataframe[which(meta_dataframe$requester_received_pizza == FALSE),]$request_title
successful_requests_text <- meta_dataframe[which(meta_dataframe$requester_received_pizza == TRUE),]$request_text
failed_requests_text <- meta_dataframe[which(meta_dataframe$requester_received_pizza == FALSE),]$request_text
create_word_cloud(successful_requests_titles)
create_word_cloud(failed_requests_titles)
create_word_cloud(successful_requests_text)
create_word_cloud(failed_requests_text)
create_word_cloud <- function(titles){
#   titles <- requests$request_title
new_titles <- list()
for (i in 1:length(titles)){
new_titles[[i]] <- unlist(strsplit(as.vector(titles[i]), " "))
}
all_title_words = unlist(new_titles)
all_title_words_table_df <- data.frame(table(all_title_words))
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
words_page <- "https://en.wikipedia.org/wiki/Most_common_words_in_English"
tables <- readHTMLTable(words_page)
useful_words <- tables$'NULL'$V2
useful_words = c(useful_words, "is", "Pizza", "pizza", "[Request]", "[REQUEST]", "[request]", "-")
all_title_words_table_df <- all_title_words_table_df[-which(all_title_words_table_df$all_title_words %in% useful_words),]
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
#This gives us something to look at and play with. Now we try using the text mining package to
#create a word cloud.
for_word_cloud_corpus <- Corpus(VectorSource(all_title_words_table_df$all_title_words))
#cleaning
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stripWhitespace)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, tolower)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, PlainTextDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, stopwords("english"))
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stemDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, c("[request]", "request"))
wordcloud(for_word_cloud_corpus, scale = c(4, 1), max.words = 20, colors = brewer.pal(8, "Dark2"))
}
create_word_cloud(successful_requests_text)
create_word_cloud <- function(titles){
#   titles <- requests$request_title
new_titles <- list()
for (i in 1:length(titles)){
new_titles[[i]] <- unlist(strsplit(as.vector(titles[i]), " "))
}
all_title_words = unlist(new_titles)
all_title_words_table_df <- data.frame(table(all_title_words))
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
words_page <- "https://en.wikipedia.org/wiki/Most_common_words_in_English"
tables <- readHTMLTable(words_page)
useful_words <- tables$'NULL'$V2
useful_words = c(useful_words, "is", "Pizza", "pizza", "[Request]", "[REQUEST]", "[request]", "-")
all_title_words_table_df <- all_title_words_table_df[-which(all_title_words_table_df$all_title_words %in% useful_words),]
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
#This gives us something to look at and play with. Now we try using the text mining package to
#create a word cloud.
for_word_cloud_corpus <- Corpus(VectorSource(all_title_words_table_df$all_title_words))
#cleaning
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stripWhitespace)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, tolower)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, PlainTextDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, stopwords("english"))
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stemDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, c("[request]", "request"))
wordcloud(for_word_cloud_corpus, scale = c(4, 1), max.words = 40, colors = brewer.pal(8, "Dark2"))
}
create_word_cloud(successful_requests_text)
create_word_cloud(failed_requests_text)
clear
head(meta_dataframe)
clear
words_page <- "https://en.wikipedia.org/wiki/Most_common_words_in_English"
tables <- readHTMLTable(words_page)
useful_words <- tables$'NULL'$V2
useful_words
tables
words_page
tables <- readHTMLTable(words_page)
tables
clear
?readHTMLTable
tables <- readHTMLTable(words_page)
tables <- readHTMLTable("https://en.wikipedia.org/wiki/Most_common_words_in_English")
library('XML')
tables <- readHTMLTable("https://en.wikipedia.org/wiki/Most_common_words_in_English")
readHTMLTable('http://www.disastercenter.com/crime/iacrime.htm')
tables <- readHTMLTable("https://en.wikipedia.org/wiki/Most_common_words_in_English")
library('RCurl')
tables <- readHTMLTable("https://en.wikipedia.org/wiki/Most_common_words_in_English")
tables <- readHTMLTable("http://en.wikipedia.org/wiki/Most_common_words_in_English")
tables <- readHTMLTable("https://en.wikipedia.org/wiki/Most_common_words_in_English")
words_page <- getURL("https://en.wikipedia.org/wiki/Most_common_words_in_English")
tables <- readHTMLTable(words_page)
tables
create_word_cloud <- function(titles){
#   titles <- requests$request_title
new_titles <- list()
for (i in 1:length(titles)){
new_titles[[i]] <- unlist(strsplit(as.vector(titles[i]), " "))
}
all_title_words = unlist(new_titles)
all_title_words_table_df <- data.frame(table(all_title_words))
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
words_page <- getURL("https://en.wikipedia.org/wiki/Most_common_words_in_English")
tables <- readHTMLTable(words_page)
useful_words <- tables$'NULL'$V2
useful_words = c(useful_words, "is", "Pizza", "pizza", "[Request]", "[REQUEST]", "[request]", "-")
all_title_words_table_df <- all_title_words_table_df[-which(all_title_words_table_df$all_title_words %in% useful_words),]
all_title_words_table_df <- all_title_words_table_df[order(all_title_words_table_df$Freq),]
#This gives us something to look at and play with. Now we try using the text mining package to
#create a word cloud.
for_word_cloud_corpus <- Corpus(VectorSource(all_title_words_table_df$all_title_words))
#cleaning
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stripWhitespace)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, tolower)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, PlainTextDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, stopwords("english"))
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, stemDocument)
for_word_cloud_corpus <- tm_map(for_word_cloud_corpus, removeWords, c("[request]", "request"))
wordcloud(for_word_cloud_corpus, scale = c(4, 1), max.words = 40, colors = brewer.pal(8, "Dark2"))
}
successful_requests_titles <- meta_dataframe[which(meta_dataframe$requester_received_pizza == TRUE),]$request_title
failed_requests_titles <- meta_dataframe[which(meta_dataframe$requester_received_pizza == FALSE),]$request_title
successful_requests_text <- meta_dataframe[which(meta_dataframe$requester_received_pizza == TRUE),]$request_text
failed_requests_text <- meta_dataframe[which(meta_dataframe$requester_received_pizza == FALSE),]$request_text
create_word_cloud(successful_requests_titles)
